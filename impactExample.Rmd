---
title: "impact example"
author: "Nina Zumel"
date: "October 18, 2016"
output: 
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

## Install WVPlots from github
# devtools::install_github('WinVector/WVPlots',build_vignettes=TRUE)

## Install sigr from github
# devtools::install_github('WinVector/sigr')

library(ggplot2)
library(WVPlots)
library(dplyr)
library(vtreat)
```

Build data

```{r}
set.seed(3424572)
Nz = 100
zip <- paste0('z',format(1:Nz, justify="right"))
zip = gsub(' ', '0', zip, fixed=TRUE)

N = 1000

zipvar = sample(zip,N,replace=TRUE)
price = rlnorm(N, meanlog=log(200000))

# "popularity" of house dependent only on price
noisemean = -mean(log(1/price))
popularity = log(1/price) + rnorm(N, mean = noisemean)

sold = popularity > 0
# add more unexplained noise -- random flips
sold = ifelse(runif(N) < 0.2, !sold, sold)

df = data.frame(zip=zipvar, price = price, sold=sold)

gp = sample(c("train", "cal", "test"), size=N, replace=TRUE, prob=c(0.4, 0.4, 0.2))

train = df[gp=="train",]
cal = df[gp=="cal", ]
test = df[gp=="test",]
```

## Naive

```{r}
bigtrain = rbind(train, cal)

treatplan.naive = vtreat::designTreatmentsC(bigtrain, "zip", 
                          outcomename="sold", 
                          outcometarget=TRUE, verbose=FALSE)

treatplan.naivesmall = vtreat::designTreatmentsC(train, "zip", 
                          outcomename="sold", 
                          outcometarget=TRUE, verbose=FALSE)

bigtrain$zip_impact = vtreat::prepare(treatplan.naive, bigtrain, pruneSig=NULL)[,"zip_catB"]
train$zip_impact = vtreat::prepare(treatplan.naive, train, pruneSig=NULL)[,"zip_catB"]
# for a downstream example
train$zip_impactsmall = vtreat::prepare(treatplan.naivesmall, train, pruneSig=NULL)[,"zip_catB"]
test$zip_impact = vtreat::prepare(treatplan.naive, test, pruneSig=NULL)[,"zip_catB"]

```

Build a model and evaluate. 
```{r}
model = glm(sold ~ log(price)+zip_impact, data=bigtrain, family=binomial)
summary(model)
# just evaluate on train set, to make compatible with next experiment
train$pred = predict(model, newdata=train, type="response")
ROCPlot(train, "pred", "sold", TRUE, title="Performance on Train")

test$predNaive = predict(model, newdata=test, type="response")
ROCPlot(test, "predNaive", "sold", TRUE, title="Performance on Test")

WVPlots::ROCPlotPair2("Train", train, "pred", "sold", TRUE,
                      "Test", test, "predNaive", "sold", TRUE,
                      "Compare model test and train performance")

```

Show the one-variable models (on training data)
```{r}
report = function(model) {
  complexity = model$df.null-model$df.residual
  devratio = model$deviance/model$null.deviance
  varexp = 100*(1-devratio)
   pVal = sigr::formatChiSqTest(model, pSmallCutoff=1e-12, format='ascii')$pValue
  print(paste("Complexity (degrees of freedom):", complexity))
  print(paste0("% deviance explained: ", format(varexp, digits=3),"%"))
  print(paste("p-value on Chi^2 Test on model:", format(pVal, digits=3)))
   #print(summary(model)) 
}
```

**True Complexity of ZIP variable**
```{r}
mod1 = glm(sold~zip, data=train, family=binomial)
report(mod1)
```

**Apparent Complexity of ZIP (`zip-impact`)**

The variance explained is lower, because the impact models were trained on `bigtrain`, not `train`, which actually reduces the overfitting. If we train the impact model on `train` then the results are nearly identical to `mod1`.

```{r}
mod2 = glm(sold~zip_impact, data=train, family=binomial)
report(mod2)

mod2 = glm(sold~zip_impactsmall, data=train, family=binomial)
report(mod2)

```

## Calibration Set
Now use a separate data set to build the impact model.

```{r}

treatplan.cal = vtreat::designTreatmentsC(cal, "zip", 
                          outcomename="sold", 
                          outcometarget=TRUE, verbose=FALSE)

train$zip_impact = vtreat::prepare(treatplan.cal, train, pruneSig=NULL)[,"zip_catB"]
test$zip_impact = vtreat::prepare(treatplan.cal, test, pruneSig=NULL)[,"zip_catB"]


model2 = glm(sold ~ log(price)+zip_impact, data=train, family=binomial)
summary(model2)
train$predCal = predict(model2, newdata=train, type="response")
ROCPlot(train, "predCal", "sold", TRUE, title="Performance on Train")

test$predCal = predict(model2, newdata=test, type="response")
ROCPlot(test, "predCal", "sold", TRUE, title="Performance on Test")

ROCPlotPair(test, "predCal", "predNaive", "sold", TRUE, title="Compare Naive Coding to Calibration Set")
```

## Cross-Frame
Use cross-validation to build the models
```{r}
cfe = vtreat::mkCrossFrameCExperiment(bigtrain, "zip", "sold", TRUE)

bigtrain$zip_impact = cfe$crossFrame$zip_catB
train$zip_impact = vtreat::prepare(cfe$treatments, train, pruneSig=NULL)[,"zip_catB"]
test$zip_impact = vtreat::prepare(cfe$treatments, test, pruneSig=NULL)[,"zip_catB"]

model = glm(sold ~ log(price)+zip_impact, data=bigtrain, family=binomial)
summary(model)
# just evaluate on train set, to make compatible with next experiment
train$predX = predict(model, newdata=train, type="response")
ROCPlot(train, "predX", "sold", TRUE, title="Cross Frame Performance on Train")

test$predX = predict(model, newdata=test, type="response")
ROCPlot(test, "predX", "sold", TRUE, title="Cross Frame Performance on test")

ROCPlotPair(test, "predX", "predNaive", "sold", TRUE, title="Compare Naive Coding to Cross Frame")
```

## DiffPriv
Use Laplace noise differential privacy to build the models.  We just chose
a typical noising rate instead of searching for an optimal rate on 
held-out or cross-validated data.

```{r}

# See: http://www.win-vector.com/blog/2015/10/a-simpler-explanation-of-differential-privacy/
rlaplace <- function(n,sigma) {
  if(sigma<=0) {
    return(numeric(n))
  }
  rexp(n,rate = 1/sigma) - rexp(n,rate = 1/sigma)
}

# add in pseudo-observations to get diff priv Laplace noising into counts
bigtrainnoised <- bigtrain[,c('zip','sold')]
bigtrainnoised$wts <- 1
levels <- unique(bigtrainnoised$zip)
sigma <- 10
synObs <- do.call(rbind,lapply(levels,
                               function(li) {
                                 data.frame(zip=li,
                                            sold=c(TRUE,FALSE),
                                            wts=c(max(0,rlaplace(1,sigma)),
                                                  max(0,rlaplace(1,sigma))))
                               }))

# append the synthetic observations to the training data
bigtrainnoised <- rbind(bigtrainnoised,synObs)

# the treatment plan uses the noised observations to impact code
treatplan.diffpriv = vtreat::designTreatmentsC(bigtrainnoised, "zip", 
                                               weights=bigtrainnoised$wts,
                          outcomename="sold", 
                          outcometarget=TRUE, verbose=FALSE)

bigtrain$zip_impact = vtreat::prepare(treatplan.diffpriv, bigtrain, pruneSig=NULL)[,"zip_catB"]
train$zip_impact = vtreat::prepare(treatplan.diffpriv, train, pruneSig=NULL)[,"zip_catB"]
test$zip_impact = vtreat::prepare(treatplan.diffpriv, test, pruneSig=NULL)[,"zip_catB"]


model = glm(sold ~ log(price)+zip_impact, data=bigtrain, family=binomial)
summary(model)

train$predD = predict(model, newdata=train, type="response")
ROCPlot(train, "predD", "sold", TRUE, title="Diff Priv Performance on Train")

test$predD = predict(model, newdata=test, type="response")
ROCPlot(test, "predD", "sold", TRUE, title="Diff Priv Performance on test")

ROCPlotPair(test, "predD", "predNaive", "sold", TRUE, 
            title="Compare Naive Coding to Diff Priv")
```
